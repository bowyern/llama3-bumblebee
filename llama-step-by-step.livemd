# Llama3 Up and Running

```elixir
Mix.install([
  {:bumblebee, "~> 0.5.3"},
  {:nx, "~> 0.7.1"},
  {:exla, "~> 0.7.1"},
  {:kino, "~> 0.12.3"},
  {:kino_bumblebee, "~> 0.5.0"}
])

Nx.global_default_backend({EXLA.Backend, client: :cuda})
```

## Section

```elixir
EXLA.Client.get_supported_platforms()
```

```elixir
hf_token = System.fetch_env!("HF_TOKEN")
repo = {:hf, "meta-llama/Meta-Llama-3-8B-Instruct", auth_token: hf_token}

{:ok, model_info} =
  Bumblebee.load_model(repo, backend: {EXLA.Backend, client: :cuda}, type: :bf16)

{:ok, tokenizer} = Bumblebee.load_tokenizer(repo)

tokenizer =
  tokenizer
  |> Map.put(:special_tokens, %{
    pad: "<|eot_id|>",
    bos: "<|begin_of_text|>",
    eos: "<|eot_id|>",
    unk: "<unk>"
  })

{:ok, generation_config} = Bumblebee.load_generation_config({:local, "./"})

generation_config =
  Bumblebee.configure(generation_config,
    max_new_tokens: 500,
    strategy: %{type: :multinomial_sampling, top_p: 0.6}
  )

serving =
  Bumblebee.Text.generation(model_info, tokenizer, generation_config,
    compile: [batch_size: 1, sequence_length: 1028],
    stream: true,
    defn_options: [compiler: EXLA]
  )

Kino.start_child({Nx.Serving, name: Llama, serving: serving})
```

```elixir
user = "What do you know about elixir?"

prompt = """
<|begin_of_text|>
<|start_header_id|>system<|end_header_id>

You are a helpful assistant.<|eot_id>
<|start_header_id|>user<|end_header_id>

#{user}<|eot_id>
<|start_header_id|>assistant<|end_header_id|>

"""

Nx.Serving.batched_run(Llama, prompt) |> Enum.each(&IO.write/1)
```
